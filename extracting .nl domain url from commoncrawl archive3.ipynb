{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting NL Domain URLs from CommonCrawl Archive\n",
    "#### *1. Using CommonCrawl [Index Client](http://index.commoncrawl.org) (CDX Index API Client)*\n",
    "  * For retrieving the URLâ€™s of the Dutch Domain in the Common Crawl achieve of September, the [cdx-index-client](https://github.com/ikreymer/cdx-index-client) can be used in [cygwin64](https://www.cygwin.com) terminal (a linux like environment for windows) it can also be used in the Jupyter environment as shown below.<br>\n",
    "  * The following code is used download gz files from the September 2016 archive: `./cdx-index-client.py -c CC-MAIN-2016-40 *.nl --fl url -z` (Changing 2016-40 to 2017-13 gives the March 2017 archive).(`!cdx-index-client.py -c CC-MAIN-2016-40 *.nl --fl url -z`   in Jupyter.)  <br>\n",
    "  * The cdx-index-client ran with python3 does gives a couple of errors. These could be easily by changing some bits of code (mainly typos and capitals).<br>\n",
    "  * The cdx-index-client downloaded 552 gz files which all contain several URLS. Many of the Urls not only contain the hosting address but also an additional path, e.g., the http://www.doof.nl has more than a hundred URLS in the the 100th gz file.<br>\n",
    "  * Total runningtime for downloading the urls from the September 2016 Common Crawl archive was: 52 minutes.<br>\n",
    "  * Total runningtime for downloading the urls from the March 2017 Common Crawl archive was: 3 hours and 16 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cdx-index-client.py -c CC-MAIN-2016-40 *.nl --fl url -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2. Funtions for extracting unique URLS from gz files*\n",
    "\n",
    "Two functions are created to extract urls from the gz files. The extracturls functions and the create functions. The create function will be used in the extracturls function and thus only the extracturls function needs to be ran.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Extracturls Function\n",
    "\n",
    "The purpose of the extracturls function is to extract the urls from the gz files which are downloaded with the cdx-index-client and return a txt file with unique urls. Beside this it will give the total running time the total urls extracted, average urls per gz file, total size of processed files and average size per gz file. \n",
    "\n",
    "The function six optional arguments. With the **'path'** argument a directory in which the gz files are located can be choosen. The argument **'name'** makes it possible for the user to choose a name for the final file(s) (e.g. september_2016_urls.csv) when no name is given by user it will automatically choose **'output'** as name for the file. Another optional argument is **'prnt'**, by setting it to *True* (prnt = *True*) for each gz files processed it will print running time per file, the amount of urls extractied from it and the filesize of the url. \n",
    "\n",
    "Finally the three arguments txt, json and csv enables the user to choose a format for the outputfile **csv** is set to *True* by default, while **json** and **txt** are set to *False* by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extracturls(path = \"\", name = 'output', prnt = False, csv = True, txt = False, json = False):\n",
    "    import glob\n",
    "    import time\n",
    "    import json as js\n",
    "    import builtins # is used because conflict with gzip open.\n",
    "    import os\n",
    "    import csv as cs\n",
    "    from gzip import open\n",
    "    if len(path) == 0:\n",
    "        path = os.getcwd()\n",
    "        print(\"Using files in working directory\")\n",
    "    urls =[] # creates empty list for list complete list of urls.\n",
    "    start_time0 = time.time()\n",
    "    N = len(glob.glob1(path,\"*.gz\")) # counts the how many gz files are in the directory.\n",
    "    name += \"_\"+str(N) #adds number of gz files to name.\n",
    "    total_size = 0\n",
    "    for i in range(N):\n",
    "        start_time = time.time() #starts counting time for each loop (.gz file)\n",
    "        clean_urls = []  #create empty lists for cleaned urls for each loop.\n",
    "        j = ((len(str(N))-len(str(i)))*'0')+str(i) #This line determines the how many 0's if any should be added to file name.\n",
    "        f = open(path+'\\domain-nl-'+j+'.gz', mode='rt') # Any streaming file object that supports `read`\n",
    "        size = os.stat(path+'\\domain-nl-'+j+'.gz').st_size\n",
    "        url = f.read().split() # spilts into seperate urls.  \n",
    "        for k in url: # iterate through seperated urls.\n",
    "            clean_url = k[k.find(\"http\"):k.find(\".nl\")+3] #only selects first two segments of urls.\n",
    "            clean_urls.append(clean_url) # append to clean_urls.\n",
    "        total_size += size\n",
    "        urls.extend(clean_urls) #extends complete list with clean_urls.\n",
    "        #creating an empty list for each loop and copy results into a second complete list\n",
    "        #dramatically decreases running time since the urls to check do not increase after each loop\n",
    "        #otherwise running time increases exponentially. \n",
    "        runningtime = (time.time() - start_time) #calculates running time.\n",
    "        if prnt == True:\n",
    "            print(\"runningtime {}:\".format(i),  round(runningtime,2),\"seconds,\", len(clean_urls), 'URL\\'s', 'filesize', round(size/1024,2), 'KB')\n",
    "        #m, s = divmod(runningtime, 60) # separates seconds into seconds and minutes.\n",
    "        #h, m = divmod(m, 60) # separates minutes into minutes and hours.\n",
    "        #print (\" Running time {} = %d:%02d:%02d\".format(i) % (h, m, s)) #gives running total running time.\n",
    "    uniqueurls = list(set(urls))#removes duplicates, but distorts the order. \n",
    "    uniqueurls.sort()\n",
    "    create(name, uniqueurls, txt, json, csv)\n",
    "    runningtime0 = (time.time() - start_time0) #gets total running time.\n",
    "    m, s = divmod(runningtime0,60) # separates seconds into seconds and minutes.\n",
    "    h, m = divmod(m, 60) # separates minutes into minutes and hours.\n",
    "    #uniqueurls2.sort() #puts in alphabethical order.\n",
    "    print (\"Total Runningtime: %d:%02d:%02d\" % (h, m, s)) #gives running total running time.\n",
    "    print (\"Total # Urls {}, Avarage Urls/file {}\".format(len(urls), (len(urls)/N)))\n",
    "    print (\"Total Size {} MB, Average filesize {} KB\".format((round((total_size/(1024*1024)),2)) , (round((total_size/ N)/(1024),2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Create Function\n",
    "\n",
    "The create function will use the arguments from the extracturls function in order to output the in format which the user has chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create(name,X,txt, json, csv): \n",
    "    import time\n",
    "    import builtins\n",
    "    import json as js\n",
    "    import os\n",
    "    import csv as cs\n",
    "    while True:\n",
    "        if txt == True:\n",
    "            print(\"creating txt file\")\n",
    "            output = builtins.open(name+'.txt','w') #creates a txt file.\n",
    "            js.dump(X,output) #outputs the created urls list to created json file.\n",
    "            output.close()\n",
    "            print (\"textfile {} created\".format(name+'.txt'))\n",
    "            size = os.stat(r'C:\\Users\\Eltebook 01\\Documents\\CBS Text mining\\gzipstream-master\\{}.txt'.format(name)).st_size\n",
    "            print (round((size/1024)/1024,2), 'MB')\n",
    "            txt = False\n",
    "            print(txt)\n",
    "        if json == True:\n",
    "            print(\"creating json file\")\n",
    "            output = builtins.open(name+'.json','w') #creates a json file.\n",
    "            js.dump(X,output) #outputs the created urls list to created csv file.\n",
    "            output.close()\n",
    "            print (\"jsonfile {} created\".format(name+'.json'))\n",
    "            size = os.stat(r'C:\\Users\\Eltebook 01\\Documents\\CBS Text mining\\gzipstream-master\\{}.json'.format(name)).st_size\n",
    "            print (round((size/1024)/1024,2), 'MB')\n",
    "            js = False\n",
    "        if csv == True:  \n",
    "            print(\"creating csv file\")\n",
    "            with builtins.open(name+'.csv', 'w') as output:\n",
    "                writer = cs.writer(output)\n",
    "                writer.writerows(X)\n",
    "            output.close()\n",
    "            print (\"csvfile {} created\".format(name+'.csv'))\n",
    "            size = os.stat(r'C:\\Users\\Eltebook 01\\Documents\\CBS Text mining\\gzipstream-master\\{}.csv'.format(name)).st_size\n",
    "            print (round((size/1024)/1024,2), 'MB')\n",
    "            csv = False\n",
    "        else:\n",
    "            break\n",
    "    print(\"files created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total urls in the September 2016 archive: **8261874** <br>\n",
    "Total unique urls in the September 2016 archive: **635740** <br><br>\n",
    "Total urls in March 2017 archive: **25127201**<br>\n",
    "Total unique urls in the March archive: **1117424**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *3. Running the Program*\n",
    "\n",
    "Running the codeblock below will create by defealt creates a csv file with unique urls extracted from the gz files in the working directory. By setting txt to True (extracturls(`txt = True`) will create a csv file and a txt file. If only a json file is to be created use (extracturls(csv = False, json = True)\n",
    "\n",
    "If extracturls(X) is ran it will produce a csv file from path X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files in working directory\n",
      "creating csv file\n",
      "csvfile output_552.csv created\n",
      "33.98 MB\n",
      "files created\n",
      "Total Runningtime: 0:01:50\n",
      "Total # Urls 8261874, Avarage Urls/file 14967.16304347826\n",
      "Total Size 132.25 MB, Average filesize 245.33 KB\n"
     ]
    }
   ],
   "source": [
    "X = r'C:\\Users\\Eltebook 01\\Documents\\CBS Thesis\\.nl domain' \n",
    "extracturls()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
